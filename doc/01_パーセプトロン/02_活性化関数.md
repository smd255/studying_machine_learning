
# 📘 活性化関数について

## 🔹 活性化関数とは？

まずおさらいですが、活性化関数は：

> **「重み付き入力の合計（線形な値）」を、**
> \*\*「モデルが複雑なパターンを学習できるように変換する関数」\*\*です。

非線形な活性化関数を使うことで、ニューラルネットは線形モデルを超えた表現力を持てるようになります。

---

## ① Sigmoid関数

### 📌 数式


$\sigma(x) = \frac{1}{1 + e^{-x}}$

### 📈 グラフの特徴

* 出力範囲： $(0, 1)$
* S字型の滑らかなカーブ
* $x = 0$ のとき、出力は 0.5
* 入力が大きくなると1に近づき、小さくなると0に近づく

![sigmoid graph](https://upload.wikimedia.org/wikipedia/commons/8/88/Logistic-curve.svg)

### ✅ メリット

* **確率的な出力が欲しいとき**（例：分類の確信度）
* ニューロンが「発火する／しない」という意味づけに使いやすい
* 滑らかで微分可能

### ⚠️ デメリット

* **勾配消失問題**（Vanishing Gradient）
  → 入力が極端に大きい／小さいと、微分がほぼ0になる
* 出力の平均が 0 にならない → 学習が非効率になることがある

---

## ② ReLU（Rectified Linear Unit）

### 📌 数式

$$
f(x) = \max(0, x)
$$

### 📈 グラフの特徴

* 出力範囲： $[0, \infty)$
* $x > 0$ のとき、出力はそのまま
* $x \leq 0$ のとき、出力は0

![ReLU graph](https://upload.wikimedia.org/wikipedia/commons/6/6c/Rectifier_and_softplus_functions.svg)

（青い線がReLU）

### ✅ メリット

* 非線形なのに非常に**計算が軽い**
* **勾配消失問題が起きにくい**
* 実際の深層学習（CNNなど）で最も広く使われている

### ⚠️ デメリット

* **死んだReLU問題（Dead ReLU）**
  → 入力が負の値だと、勾配も出力も常に0になってしまい、
  そのユニットは学習されなくなることがある

---

## 🔄 比較まとめ表

| 特性       | Sigmoid         | ReLU         |
| -------- | --------------- | ------------ |
| 出力範囲     | (0, 1)          | \[0, ∞)      |
| 非線形性     | あり              | あり           |
| 勾配消失のリスク | 高い（特に端の方で）      | 低い           |
| 計算コスト    | やや高い（指数関数）      | 非常に軽い（単純な比較） |
| 出力の中心    | 0.5周辺（0に中心ではない） | 正側のみ         |
| よく使う場面   | 出力層（確率的な分類）     | 隠れ層（深層学習の標準） |

---

## ✨ 補足：どちらを使うべき？

* \*\*隠れ層（中間の層）\*\*では、基本的に **ReLU**（または改良版：LeakyReLU, GELUなど）が一般的。
* **出力層**では、分類タスクでクラスごとの確率が必要な場合、\*\*Sigmoid（2クラス）**や**Softmax（多クラス）\*\*が使われます。

---

## 💡 まとめ

* Sigmoid：0～1の滑らかな出力。確率的な表現に向くが勾配消失に注意。
* ReLU：シンプルで強力な非線形関数。深層学習では定番。ただし死にユニットに注意。

---
