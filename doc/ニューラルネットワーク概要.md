---

## 📘 ニューラルネットワークの基本理解と応用まとめ（Markdown）

---

## ✅ ニューラルネットワークの構造（基本理解）

* 入力層、中間層、出力層がある。
* 中間層では、**重み付き和と活性化関数**による処理が行われる。
* 中間層の計算結果は、重み・バイアス・活性化関数によって様々に変化する。
* この非線形処理により、複雑なデータ構造やパターンが学習できる。

---

## 🔢 数式で見るニューラルネットワーク

### ▶ 単一ニューロンの計算式

$$
z = w_1 x_1 + w_2 x_2 + \cdots + w_n x_n + b
$$

$$
a = \phi(z)
$$

* $w_i$：重み
* $x_i$：入力
* $b$：バイアス
* $\phi(z)$：活性化関数を通した出力

---

### ▶ 単純なネットワーク全体（1中間層）

$$
\boldsymbol{x} = \begin{bmatrix} x_1 \\ x_2 \\ x_3 \end{bmatrix}
$$

$$
\boldsymbol{z}^{(1)} = \boldsymbol{W}^{(1)} \boldsymbol{x} + \boldsymbol{b}^{(1)}
$$

$$
\boldsymbol{a}^{(1)} = \phi(\boldsymbol{z}^{(1)})
$$

$$
y = \boldsymbol{w}^{(2)} \cdot \boldsymbol{a}^{(1)} + b^{(2)}
$$

---

## 🖼️ ネットワーク構造（図イメージ）

```
入力層（x1, x2, x3）
   │     │     │
   ▼     ▼     ▼
  ◯────▶◯────▶◯　← 中間層（Hidden layer）
   \     │     /
    \    ▼    /
     ──▶◯────    ← 出力層（Output y）
```

---

## 🔧 活性化関数の役割と種類

### ● 役割

* 非線形変換を加えることで、**複雑な関係性を学習可能にする**。
* 活性化関数がないと、複数層でも結局は線形変換と同じ。

### ● よく使われる活性化関数

| 関数名     | 数式                                           | 特徴                   | よく使う場面      |
| ------- | -------------------------------------------- | -------------------- | ----------- |
| ReLU    | $\phi(x) = \max(0, x)$                       | 高速・スパース・勾配消失しにくい     | 中間層         |
| Sigmoid | $\phi(x) = \frac{1}{1 + e^{-x}}$             | 出力0〜1・古典的・勾配消失しやすい   | 出力層（二値分類）   |
| tanh    | $\phi(x) = \tanh(x)$                         | 出力-1〜1・Sigmoidより良い性質 | 中間層（小規模モデル） |
| Softmax | $\phi_i(x) = \frac{e^{x_i}}{\sum_j e^{x_j}}$ | 確率解釈・多クラス分類に必須       | 出力層（多クラス分類） |

---

## 🔄 誤差逆伝播法（Backpropagation）

### ● 目的

* \*\*各重みが損失にどれだけ影響するか（微分）\*\*を計算し、最適化のための勾配を得る。

### ● 処理の流れ

1. **順伝播（Forward）**
   $\hat{y} = f(\boldsymbol{x})$

2. **損失計算**
   $L = \text{loss}(\hat{y}, y)$

3. **誤差逆伝播（Backprop）**

   $$
   \frac{\partial L}{\partial w} = \frac{\partial L}{\partial a} \cdot \frac{\partial a}{\partial z} \cdot \frac{\partial z}{\partial w}
   $$

4. **重み更新（Gradient Descent）**

   $$
   w := w - \eta \cdot \frac{\partial L}{\partial w}
   $$

---

### ● 図によるイメージ

```
入力 → 中間層 → 出力（Forward）
         ↓
     損失を計算
         ↓
出力 → 中間層 → 入力へ（Backward）
（各重みを勾配に従って調整）
```

---

## 🌱 次のステップ候補

* 損失関数の選び方（MSE, クロスエントロピーなど）
* 勾配消失・勾配爆発の対策
* 最適化アルゴリズム（SGD, Adamなど）
* 実装（PyTorch / TensorFlow）

---

